{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to evaluate Longformer on TriviaQA using NLP",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMa7uHx4GZoJO+I3xWVwPfx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbNZdYkugq7-",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of a model using NLP\n",
        "\n",
        "*This notebook shows how `nlp` can be leveraged `nlp` to evaluate **Longformer** on **TriviaQA** .*\n",
        "\n",
        "- The [`nlp`](https://github.com/huggingface/nlp) library allows simple and intuitive access to nlp datasets and metrics.\n",
        "\n",
        "- **Longformer** is transformer-based model for long-range sequence modeling introduced by *Iz Beltagy, Matthew E. Peters, Arman Cohan* (see paper [here](https://arxiv.org/abs/2004.05150)) and can now be accessed via ü§ó Transformers via the [docs](https://huggingface.co/transformers/model_doc/longformer.html).\n",
        "\n",
        "- **TriviaQA** is a reading comprehension dataset containing question-answer-evidence triplets (see paper here [here](https://homes.cs.washington.edu/~eunsol/papers/acl17jcwz.pdf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckDfoClXj-mm",
        "colab_type": "text"
      },
      "source": [
        "We will evaluate a pretrained `LongformerForQuestionAnswering` model on the *validation* dataset of **TriviaQA**. Along the way, this notebook will show you how `nlp` can be used for effortless preprocessing of data and analysis of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGg2btE2mY0v",
        "colab_type": "text"
      },
      "source": [
        "Alright! Let's start by installing the `nlp` library and loading **TriviaQA**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHkpUVuft4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "474ee83e-6056-4b32-a139-d8351514258e"
      },
      "source": [
        "# install nlp\n",
        "# ATTENTION. Rerunning this command remove the cached trivia qa dataset completely \n",
        "!rm -rf /root/.cache/huggingface/datasets/\n",
        "!pip uninstall -y -qq pyarrow\n",
        "!pip uninstall -y -qq nlp\n",
        "!pip install -qq git+https://github.com/huggingface/nlp.git\n",
        "!pip install -qq git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "# import nlp\n",
        "import nlp"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpOqRdcjnaBT",
        "colab_type": "text"
      },
      "source": [
        "The total **TriviaQA** dataset has a size of 17 GB once processed.\n",
        "Downloading and preprocessing the dataset will take around *15 minutes*.\n",
        "# ‚òï\n",
        "Afterwards the data is serialized in *parquet* format for quick reloading via *Arrow*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YusOKo5FgkYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_dataset = nlp.load_dataset(\"trivia_qa\", \"rc\", split=\"validation[:1%]\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_LFkvD_sx9H",
        "colab_type": "text"
      },
      "source": [
        "First, let's get an overview of the dataset üßê"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJTko97rwQGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a344527a-4337-498c-cdc2-09459e7948b8"
      },
      "source": [
        "validation_dataset"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(schema: {'question': 'string', 'question_id': 'string', 'question_source': 'string', 'entity_pages': 'struct<doc_source: list<item: string>, filename: list<item: string>, title: list<item: string>, wiki_context: list<item: string>>', 'search_results': 'struct<description: list<item: string>, filename: list<item: string>, rank: list<item: int32>, title: list<item: string>, url: list<item: string>, search_context: list<item: string>>', 'answer': 'struct<aliases: list<item: string>, normalized_aliases: list<item: string>, matched_wiki_entity_name: string, normalized_matched_wiki_entity_name: string, normalized_value: string, type: string, value: string>'}, num_rows: 187)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHsmt1ULwTug",
        "colab_type": "text"
      },
      "source": [
        "1% of the validation data corresponds to 187 examples, which we can use as a good snapshot of the actual dataset and get be used to get familiar with the dataset.\n",
        "\n",
        "Let's check out the datatset's structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjAVcrAitsU5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "19123abb-5d52-48bb-de45-4854ec83a371"
      },
      "source": [
        "# check out schema\n",
        "validation_dataset.schema"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question: string not null\n",
              "question_id: string not null\n",
              "question_source: string not null\n",
              "entity_pages: struct<doc_source: list<item: string>, filename: list<item: string>, title: list<item: string>, wiki_context: list<item: string>> not null\n",
              "  child 0, doc_source: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 1, filename: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 2, title: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 3, wiki_context: list<item: string>\n",
              "      child 0, item: string\n",
              "search_results: struct<description: list<item: string>, filename: list<item: string>, rank: list<item: int32>, title: list<item: string>, url: list<item: string>, search_context: list<item: string>> not null\n",
              "  child 0, description: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 1, filename: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 2, rank: list<item: int32>\n",
              "      child 0, item: int32\n",
              "  child 3, title: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 4, url: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 5, search_context: list<item: string>\n",
              "      child 0, item: string\n",
              "answer: struct<aliases: list<item: string>, normalized_aliases: list<item: string>, matched_wiki_entity_name: string, normalized_matched_wiki_entity_name: string, normalized_value: string, type: string, value: string> not null\n",
              "  child 0, aliases: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 1, normalized_aliases: list<item: string>\n",
              "      child 0, item: string\n",
              "  child 2, matched_wiki_entity_name: string\n",
              "  child 3, normalized_matched_wiki_entity_name: string\n",
              "  child 4, normalized_value: string\n",
              "  child 5, type: string\n",
              "  child 6, value: string"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XqHiM5mt5t-",
        "colab_type": "text"
      },
      "source": [
        "Alright! For Questions Answering we are intersting in the *question*, the *evidence* and the *answer*. Because **Longformer** was trained on the Wikipedia part of **TriviaQA**, we will use `validation_dataset[\"entity_pages\"][\"search_context\"]` as our evidence. \n",
        "We can also see that there are multiple answers. \n",
        "\n",
        "In this use case, we define a correct output of the model as one that is one of the answer aliases `validation_dataset[\"answer\"][\"aliases\"]`. Lastly, we also keep `validation_dataset[\"answer\"][\"normalized_value\"]`. All other columns can be disregarded. We apply the `.map()` function to map the dataset into the format as defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEkZy_LvhEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the mapping function\n",
        "def format_dataset(example):\n",
        "    # the evidence might be comprised of multiple contexts => me merge them here\n",
        "    example[\"evidence\"] = \" \".join((\"\\n\".join(example[\"entity_pages\"][\"wiki_context\"])).split(\"\\n\"))\n",
        "    example[\"targets\"] = example[\"answer\"][\"aliases\"]\n",
        "    example[\"norm_target\"] = example[\"answer\"][\"normalized_value\"]\n",
        "    return example\n",
        "\n",
        "# map the dataset and throw out all unnecessary columns\n",
        "validation_dataset = validation_dataset.map(format_dataset, remove_columns=[\"search_results\", \"question_source\", \"entity_pages\", \"answer\", \"question_id\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRhh7AJrw3fM",
        "colab_type": "text"
      },
      "source": [
        "Now, we can check out a first example of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I730mW9wNeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "4d4fdde9-c7cf-4597-8488-899d57256bd1"
      },
      "source": [
        "validation_dataset[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'evidence': 'Andrew Lloyd Webber, Baron Lloyd-Webber   (born 22 March 1948) is an English composer and impresario of musical theatre.   Several of his musicals have run for more than a decade both in the West End and on Broadway. He has composed 13 musicals, a song cycle, a set of variations, two film scores, and a Latin Requiem Mass. Several of his songs have been widely recorded and were hits outside of their parent musicals, notably \"The Music of the Night\" from The Phantom of the Opera, \"I Don\\'t Know How to Love Him\" from Jesus Christ Superstar, \"Don\\'t Cry for Me, Argentina\" and \"You Must Love Me\" from Evita, \"Any Dream Will Do\" from Joseph and the Amazing Technicolor Dreamcoat and \"Memory\" from Cats.  He has received a number of awards, including a knighthood in 1992, followed by a peerage from Queen Elizabeth II for services to Music, seven Tonys, three Grammys (as well as the Grammy Legend Award), an Academy Award, fourteen Ivor Novello Awards, seven Olivier Awards, a Golden Globe, a Brit Award, the 2006 Kennedy Center Honors, and the 2008 Classic Brit Award for Outstanding Contribution to Music.    He has a star on the Hollywood Walk of Fame, is an inductee into the Songwriter\\'s Hall of Fame, and is a fellow of the British Academy of Songwriters, Composers and Authors.   His company, the Really Useful Group, is one of the largest theatre operators in London. Producers in several parts of the UK have staged productions, including national tours, of the Lloyd Webber musicals under licence from the Really Useful Group. Lloyd Webber is also the president of the Arts Educational Schools London, a performing arts school located in Chiswick, West London. He is involved in a number of charitable activities, including the Elton John AIDS Foundation, Nordoff Robbins, Prostate Cancer UK and War Child. In 1992 he set up the Andrew Lloyd Webber Foundation which supports the arts, culture and heritage in the UK.   Early life  Andrew Lloyd Webber was born in Kensington, London, the elder son of William Lloyd Webber (1914‚Äì1982), a composer and organist, and Jean Hermione Johnstone (1921‚Äì1993), a violinist and pianist.  His younger brother, Julian Lloyd Webber, is a renowned solo cellist.  Lloyd Webber started writing his own music at a young age, a suite of six pieces at the age of nine. He also put on \"productions\" with Julian and his Aunt Viola in his toy theatre (which he built at Viola\\'s suggestion). Later, he would be the owner of a number of West End theatres, including the Palace. His aunt Viola, an actress, took him to see many of her shows and through the stage door into the world of the theatre. He also had originally set music to Old Possum\\'s Book of Practical Cats at the age of 15.  In 1965, Lloyd Webber was a Queen\\'s Scholar at Westminster School and studied history for a term at Magdalen College, Oxford, although he abandoned the course in Winter 1965 to study at the Royal College of Music and pursue his interest in musical theatre.    Professional career  Early years  Lloyd Webber\\'s first collaboration with lyricist Tim Rice was The Likes of Us, a musical based on the true story of Thomas John Barnardo. Although composed in 1965, it was not publicly performed until 2005, when a production was staged at Lloyd Webber\\'s Sydmonton Festival. In 2008, amateur rights were released by the National Operatic and Dramatic Association (NODA) in association with the Really Useful Group. The first amateur performance was by a children\\'s theatre group in Cornwall called \"Kidz R Us\". Stylistically, The Likes of Us is fashioned after the Broadway musical of the 1940s and 1950s; it opens with a traditional overture comprising a medley of tunes from the show, and the score reflects some of Lloyd Webber\\'s early influences, particularly Richard Rodgers, Frederick Loewe, and Lionel Bart. In this respect, it is markedly different from the composer\\'s later work, which tends to be either predominantly or wholly through-composed, and closer in form to opera than to the Broadway musical.  In 1968, Rice and Lloyd Webber were commissioned to write a piece for the Colet Court preparatory school, which resulted in Joseph and the Amazing Technicolor Dreamcoat, a retelling of the biblical story of Joseph in which Lloyd Webber and Rice humorously pastiche a number of musical styles such as Elvis-style rock\\'n\\'roll, Calypso and country music. Joseph began life as a short cantata that gained some recognition on its second staging with a favourable review in The Times. For its subsequent performances, Rice and Lloyd Webber revised the show and added new songs to expand it to a more substantial length. This culminated in a two-hour-long production being staged in the West End on the back of the success of Jesus Christ Superstar.  In 1969 Rice and Lloyd Webber wrote a song for the Eurovision Song Contest called \"Try It and See,\" which was not selected. With rewritten lyrics it became \"King Herod\\'s Song\" in their third musical, Jesus Christ Superstar (1970).  The planned follow-up to Jesus Christ Superstar was a musical comedy based on the Jeeves and Wooster novels by P. G. Wodehouse. Tim Rice was uncertain about this venture, partly because of his concern that he might not be able to do justice to the novels that he and Lloyd Webber so admired.  After doing some initial work on the lyrics, he pulled out of the project and Lloyd Webber subsequently wrote the musical with Alan Ayckbourn, who provided the book and lyrics. Jeeves failed to make any impact at the box office and closed after a short run of only three weeks. Many years later, Lloyd Webber and Ayckbourn revisited this project, producing a thoroughly reworked and more successful version entitled By Jeeves (1996). Only two of the songs from the original production remained (\"Half a Moment\" and \"Banjo Boy\").  Mid-1970s  Lloyd Webber collaborated with Rice once again to write Evita (1978 in London/1979 in U.S.), a musical based on the life of Eva Per√≥n. As with Jesus Christ Superstar, Evita was released first as a concept album (1976) and featured Julie Covington singing the part of Eva Per√≥n. The song \"Don\\'t Cry for Me Argentina\" became a hit single and the musical was staged at the Prince Edward Theatre in a production directed by Harold Prince and starring Elaine Paige in the title role.  Patti LuPone created the role of Eva on Broadway for which she won a Tony.  Evita was a highly successful show that ran for ten years in the West End. It transferred to Broadway in 1979. Rice and Lloyd Webber parted ways soon after Evita. In an interview in 2011, LuPone commented \"He writes crap music... Evita was his best score, Evita in its bizarreness - when I first heard it I thought \\'I swear to God, he hated women\\' [...] There are some very romantic moments in his music, and there is some real...trash that he doesn\\'t even think about parting with. He\\'s not a very good editor of his own stuff.\"   In 1978, Lloyd Webber embarked on a solo project, the \"Variations\", with his cellist brother Julian based on the 24th Caprice by Paganini, which reached number two in the pop album chart in the United Kingdom. The main theme was used as the theme tune for ITV\\'s long-running South Bank Show throughout its 32-year run. The same year, Lloyd Webber also composed a new theme tune for the long-running documentary series Whicker\\'s World, which was used from 1978-80.  1980s  Lloyd Webber was the subject of This Is Your Life in November 1980 when he was surprised by Eamonn Andrews in the foyer of Thames Television\\'s Euston Road Studios. He would be honoured a second time by the television programme in November 1994 when Michael Aspel surprised him at the Adelphi Theatre.  Lloyd Webber embarked on his next project without a lyricist, turning instead to the poetry of T. S. Eliot. Cats (1981) was to become the longest running musical in London, where it ran for 21 years before closing. On Broadway, Cats ran for 18 years, a record which would ultimately be broken by another Lloyd Webber musical, The Phantom of the Opera.   Starlight Express (1984) was a commercial hit, but received negative reviews from the critics. It enjoyed a record run in the West End, but ran for less than two years on Broadway. The show has also seen two tours of the US, as well as an Australian/Japanese production, a three-year UK touring production, which transferred to New Zealand later in 2009. The show also runs full-time in a custom-built theatre in Bochum, Germany, where it has been running since 1988.  Lloyd Webber wrote a Requiem Mass dedicated to his father, William, who had died in 1982. It premiered at St. Thomas Church in New York on 24 February 1985. Church music had been a part of the composer\\'s upbringing and the composition was inspired by an article he had read about the plight of Cambodian orphans. Lloyd Webber had on a number of occasions written sacred music for the annual Sydmonton Festival.  Lloyd Webber received a Grammy Award in 1986 for Requiem in the category of best classical composition. Pie Jesu from Requiem achieved a high placing on the UK pop charts. Perhaps because of its large orchestration, live performances of the Requiem are rare.  Cricket (1986), also called Cricket (Hearts and Wickets), reunited Lloyd Webber with Tim Rice to create this short musical for Queen Elizabeth\\'s 60th birthday, first performed at Windsor Castle. Several of the tunes were later used for Aspects of Love and Sunset Boulevard.  Lloyd Webber also premiered The Phantom of the Opera in 1986, inspired by the 1911 Gaston Leroux novel. He wrote the part of Christine for his then-wife, Sarah Brightman, who played the role in the original London and Broadway productions alongside Michael Crawford as the Phantom. The production was directed by Harold Prince, who had also earlier directed Evita. Charles Hart wrote the lyrics for Phantom with some additional material provided by Richard Stilgoe, with whom Lloyd-Webber co-wrote the book of the musical. It became a hit and is still running in both the West End and on Broadway; in January 2006 it overtook Cats as the longest-running musical on Broadway. On 11 February 2012, Phantom of the Opera played its 10,000th show on Broadway.  Aspects of Love followed in 1989, a musical based on the story by David Garnett. The lyrics were by Don Black and Charles Hart and the original production was directed by Trevor Nunn. Aspects had a run of four years in London, but closed after less than a year on Broadway. It has since gone on a tour of the UK.  1990s  Lloyd Webber was asked to write a song for the 1992 Barcelona Olympics and composed \"Amigos Para Siempre\\xa0‚Äî Friends for Life\" with Don Black providing the lyrics. This song was performed by Sarah Brightman and Jos√© Carreras.  Lloyd Webber had toyed with the idea of writing a musical based on Billy Wilder\\'s critically acclaimed movie, Sunset Boulevard, since the early 1970s when he saw the film, but the project didn\\'t come to fruition until after the completion of Aspects of Love when the composer finally managed to secure the rights from Paramount Pictures,  The composer worked with two collaborators, as he had done on Aspects of Love; this time Christopher Hampton and Don Black shared equal credit for the book and lyrics. The show opened at the Adelphi Theatre in London on 12 July 1993, and ran for 1,529 performances. In spite of the show\\'s popularity and extensive run in London\\'s West End, it lost money due to the sheer expense of the production.  In 1994, Sunset Boulevard became a successful Broadway show, opening with the largest advance in Broadway history, and winning seven Tony Awards that year. Even so, by its closing in 1997, \"it had not recouped its reported $13\\xa0million investment.\"  From 1995-2000, Lloyd Webber wrote the Matters of Taste column in The Daily Telegraph where he reviewed restaurants and hotels, and these were illustrated by Lucinda Rogers.   In 1998, Lloyd Webber released a film version of \"Cats\", which was filmed at the Adelphi Theatre in London. David Mallet directed the film, and Gillian Lynne choreographed it. The cast consisted of performers who had been in the show before, including Ken Page (the original Old Deuteronomy on Broadway), Elaine Paige (original Grizabella in London) and Sir John Mills as Gus: the Theatre Cat.  In 1998 Whistle Down the Wind made its debut, a musical written with lyrics supplied by Jim Steinman. Originally opening in Washington, Lloyd Webber was reportedly not happy with the casting or Harold Prince\\'s production and the show was subsequently revised for a London staging directed by Gale Edwards, the production is probably most notable for the number-one hit from Boyzone \"No Matter What\" which left only the UK charts when the price of the CD single was changed to drop it out of the official top ten. His The Beautiful Game opened in London and has never been seen on Broadway. The show had a respectable run at The Cambridge Theatre in London. The show has been re-worked into a new musical, The Boys in the Photograph, which had its world premi√®re at The Liverpool Institute for Performing Arts in April 2008.  2000s  Having achieved great popular success in musical theatre, Lloyd Webber was referred to by The New York Times in 2001 as \"the most commercially successful composer in history.\"   In 2002 he turned producer, bringing the musical Bombay Dreams to London. With music by Bollywood composer A.R. Rahman and lyrics by Don Black, iIt ran for two years at the Apollo Victoria Theatre. A revised Broadway production at the Broadway Theatre two years later ran for only 284 performances.  On 16 September 2004, his production of The Woman in White opened at the Palace Theatre in London. It ran for 19 months and 500 performances. A revised production opened on Broadway at the Marquis Theatre on 17 November 2005. Garnering mixed reviews from critics, due in part to the frequent absences of the show\\'s star Maria Friedman due to breast cancer treatment, it closed only a brief three months later on 19 February 2006.  Lloyd Webber produced a staging of The Sound of Music, which d√©buted November 2006. He made the controversial decision to choose an unknown to play leading lady Maria, who was found through the BBC\\'s reality television show How Do You Solve a Problem like Maria?, in which he was a judge.  The winner of the show was Connie Fisher.  It was announced on 25 August 2006, on his personal website, that his next project would be The Master and Margarita; however, it was announced in late March 2007 that he had abandoned the project.   In September 2006, Lloyd Webber was named to be a recipient of the prestigious Kennedy Center Honors with Zubin Mehta, Dolly Parton, Steven Spielberg, and Smokey Robinson. He was recognised for his outstanding contribution to American performing arts.[http://www.kennedy-center.org/programs/specialevents/honors/home.html The Kennedy Center Honors] He attended the ceremony on 3 December 2006; it aired on 26 December 2006. On 11 February 2007, Lloyd Webber was featured as a guest judge on the reality television show Grease: You\\'re the One that I Want!  The contestants all sang \"The Phantom of the Opera\".  Between April and June 2007, he appeared in BBC One\\'s Any Dream Will Do!, which followed the same format as How Do You Solve a Problem Like Maria?. Its aim was to find a new Joseph for his revival of Joseph and the Amazing Technicolor Dreamcoat. Lee Mead won the contest after quitting his part in the ensemble ‚Äì and as understudy in The Phantom of the Opera ‚Äì to compete for the role. Viewers\\' telephone voting during the series raised more than ¬£500,000 for the BBC\\'s annual Children in Need charity appeal, according to host Graham Norton on air during the final.  On 1 July 2007, Lloyd Webber presented excerpts from his musicals as part of the Concert for Diana held at Wembley Stadium, London, an event organised to celebrate the life of Princess Diana almost 10 years after her death.   BBC Radio 2 broadcast a concert of music from the Lloyd-Webber musicals on 24 August 2007.  Denise Van Outen introduced songs from Whistle Down the Wind, The Beautiful Game, Tell Me on a Sunday, The Woman in White, Evita  and Joseph and the Amazing Technicolor Dreamcoat ‚Äì as well as Rodgers and Hammerstein\\'s The Sound of Music, which Lloyd Webber revived in 2006 at the London Palladium, and the 2002 musical Bombay Dreams.  In April 2008, Lloyd Webber reprised his role as judge, this time in the BBC musical talent show I\\'d Do Anything. The show followed a similar format to its Maria and Joseph predecessors, this time involving a search for an actress to play the role of Nancy in an upcoming West End production of the Lionel Bart musical Oliver! The show also featured a search for three young actors to play and share the title character\\'s role, but the show\\'s main focus was on the search for Nancy. The role was won by Jodie Prenger despite Lloyd Webber\\'s stated preference for one of the other contestants; the winners of the Oliver role were Harry Stott, Gwion Wyn-Jones and Laurence Jeffcoate. Also in April 2008. Lloyd Webber was featured on the U.S. talent show American Idol, acting as a mentor when the 6 finalists had to select one of his songs to perform for the judges that week.   Lloyd Webber accepted the challenge of managing the UK\\'s entry for the 2009 Eurovision Song Contest, to be held in Moscow. In early 2009 a series, called Eurovision: Your Country Needs You, was broadcast to find a performer for a song that he would compose for the competition. Jade Ewen won the right to represent Britain, winning with It\\'s My Time, by Lloyd Webber and Diane Warren. At the contest, Lloyd Webber accompanied her on the piano during the performance. The United Kingdom finished 5th in the contest. The winner was Norway\\'s Alexander Rybak with his world record composition \"Fairytale\".  On 8 October 2009, Lloyd Webber launched the musical Love Never Dies at a press conference held at Her Majesty\\'s Theatre, where the original Phantom has been running since 1986. Also present were Sierra Boggess, who has been cast as Christine Daa√©, and Ramin Karimloo, who portrayed Phantom, a role he most recently played in the West End.  2010s  Following the opening of Love Never Dies, Lloyd Webber again began a search for a new musical theatre performer in the BBC One series Over the Rainbow. He cast the winner, Danielle Hope, in the role of Dorothy and a dog to play Toto in his forthcoming stage production of The Wizard of Oz. He and lyricist and composer Tim Rice wrote a number of new songs for the production to supplement the songs from the film.  On 26 February 2010, he appeared on BBC\\'s Friday Night with Jonathan Ross to promote Love Never Dies.  On 1 March 2011, The Wizard of Oz opened at The Palladium Theatre, starring Danielle Hope as Dorothy and Michael Crawford as the Wizard.  In 2012 Lloyd Webber fronted a new ITV primetime show Superstar which gave the UK public the chance to decide who would play the starring role of Jesus in an upcoming arena tour of Jesus Christ Superstar. The arena tour started in September 2012 and also starred comedian Tim Minchin as Judas Iscariot, former Spice Girl Melanie C as Mary Magdalene and BBC Radio 1 DJ Chris Moyles as King Herod.  Tickets for most venues went on sale on 18 May 2012.  Webber caused controversy with a series of comments about Eurovision in a Radio Times interview. He said: \"I don\\'t think there\\'s any point in beating around the bush. I saw no black faces on the programme Eurovision 2012. I was questioned by the press over Jade Ewen\\'s race, and I think we would have placed second, but there is a problem when you go further east. If you\\'re talking about Western Europe it\\'s fine, but Ukraine, not so good.\" The EBU corrected Webber, telling him Ukraine\\'s singer Gaitana was black, that year\\'s winner Loreen for Sweden was of North African background and accompanied by a black backing dancer, and France\\'s contestant Anggun was Indonesian. The contest organisers also told Webber that black singer Dave Benton won for Estonia in 2001. The EBU thoroughly denied racism in its show, and insisted it unites Europe for three nights in a year.  In 2013, Webber reunited with Christopher Hampton and Don Black on Stephen Ward the Musical.   In 2014, it was announced that Webber\\'s next project would be a musical adaptation of the 2003 film School of Rock.  On January 19, 2015, auditions opened for children aged nine to fifteen in cooperation with the School of Rock music education program, which predated the film by several years.   Accusations of plagiarism  Lloyd Webber has been accused of plagiarism in his works. The Dutch composer Louis Andriessen commented that: \"There are two sorts of stealing (in music) ‚Äì taking something and doing nothing with it, or going to work on what you\\'ve stolen. The first is plagiarism. Andrew Lloyd Webber has yet to think up a single note; in fact, the poor guy\\'s never invented one note by himself. That\\'s rather poor\".   However, Lloyd Webber\\'s biographer, John Snelson, countered such accusations. He acknowledged a similarity between the Andante movement of Mendelssohn\\'s Violin Concerto in E minor and the Jesus Christ Superstar song \"I Don\\'t Know How to Love Him\", but wrote that Lloyd Webber:  ...brings a new dramatic tension to Mendelssohn\\'s original melody through the confused emotions of Mary Magdalene. The opening theme may be Mendelssohn, but the rhythmic and harmonic treatment along with new lines of highly effective melodic development are Lloyd Webber\\'s. The song works in its own right as its many performers and audiences can witness.   In interviews promoting Amused to Death, Roger Waters, formerly of Pink Floyd, claimed that Lloyd Webber had copied a short chromatic riff from the 1971 song \"Echoes\" for sections of The Phantom of the Opera, released in 1986; nevertheless, he decided he did not want to file a lawsuit.  The songwriter Ray Repp also claimed that Lloyd Webber stole a different melody from his own song \"Till You\". Unlike Roger Waters, Ray Repp did decide to sue, but the court ruled in Lloyd Webber\\'s favour.   Personal life  Lloyd Webber has married three times. He married first Sarah Hugill on 24 July 1971 and they divorced on 14 November 1983. Together they had two children; a daughter and a son: *Hon. Imogen Lloyd Webber (born 31 March 1977) *Hon. Nicholas Lloyd Webber (born 2 July 1979)  He then married singer Sarah Brightman on 22 March 1984 in Hampshire. He cast Brightman in the lead role in his musical The Phantom of the Opera, among other notable roles. They divorced on 3 January 1990.  Thirdly, he married Madeleine Gurdon in Westminster on 9 February 1991. They have three children, two sons and one daughter, all of whom were born in Westminster: *Hon. Alastair Adam Lloyd Webber (born 3 May 1992) *Hon. William Richard Lloyd Webber (born 24 August 1993) *Hon. Isabella Aurora Lloyd Webber (born 30 April 1996).  The Sunday Times Rich List 2006 ranked him the 87th-richest man in Britain with an estimated fortune of ¬£700\\xa0million. His wealth increased to ¬£750\\xa0million in 2007, but the publication ranked him 101st in 2008.  He lives at Sydmonton Court, Hampshire, and owns much of nearby Watership Down.  Lloyd Webber is an art collector, with a passion for Victorian art. An exhibition of works from his collection was presented at the Royal Academy in 2003 under the title Pre-Raphaelite and Other Masters ‚Äì The Andrew Lloyd Webber Collection. In 2006, Lloyd Webber planned to sell Portrait of Angel Fern√°ndez de Soto by Pablo Picasso to benefit the Andrew Lloyd Webber Foundation.  In November 2006, he withdrew the painting from auction after a claim that the previous owner had been forced to sell it under duress in Nazi Germany.  An out-of-court settlement was reached, where the foundation retained ownership rights.  On 23 June 2010, the painting was sold at auction for ¬£34.7\\xa0million to an anonymous telephone bidder.   Lloyd Webber was made a Conservative life peer in 1997, however by the end of 2015, he had voted only 33 times.  Politically, Lloyd Webber has supported the UK\\'s Conservative Party, allowing his song \"Take That Look Off Your Face\" to be used on a party promotional film seen by an estimated 1 million people before the 2005 general election.  In 2009, he publicly criticised the Labour government\\'s introduction of a new 50% rate of income tax on Britain\\'s top earners, claiming it would damage the country by encouraging talented people to leave.  In August 2014, Lloyd Webber was one of 200 public figures who were signatories to a letter to The Guardian opposing Scottish independence in the run-up to September\\'s referendum on that issue.  In October 2015 Lloyd Webber was involved in a controversial House of Lords vote over proposed cuts to tax credits, voting with the Government in favour of the plan.    In late 2009, Lloyd Webber had surgery for early-stage prostate cancer,  but had to be readmitted to hospital with post-operative infection in November. In January 2010, he declared he was cancer-free.  He had his prostate completely removed as a preventative measure.   Honours and styles of address  Honours  Andrew Lloyd Webber was knighted by the Queen in 1992.  In 1997, he was created a life peer as Baron Lloyd-Webber, of Sydmonton in the County of Hampshire.  He is properly styled as The Lord Lloyd-Webber; the title is hyphenated, although his surname is not. He sits as a Conservative member of the House of Lords.  Styles of address  *1948‚Äì1992: Mister Andrew Lloyd Webber *1992‚Äì1997: Sir Andrew Lloyd Webber Kt *1997‚Äìpresent: The Right Honourable The Lord Lloyd-Webber Kt  Awards  Academy Awards  *1996 ‚Äì Best Original Song for \"You Must Love Me\" from Evita (award shared with Sir Tim Rice) One nomination for Best Original Song Score and Adaptation: 1973 motion picture Jesus Christ Superstar  One nomination for Best Original Song: \"Learn to Be Lonely\" from the 2004 motion picture The Phantom of the Opera .  Golden Globes  * 1997 ‚Äì Best Original Song for \"You Must Love Me\" from Evita (award shared with Sir Tim Rice) Plus one nomination for Best Original Song: \"Learn to Be Lonely\" from the 2004 motion picture The Phantom of the Opera.  Grammy Awards  *1980 ‚Äì Best Cast Show Album for Evita *1983 ‚Äì Best Cast Show Album for Cats *1986 ‚Äì Grammy Award for Best Contemporary Composition for Requiem *1990 ‚Äì Grammy Legend Award  Tony Awards  *1979 ‚Äì Best Musical for Evita *1980 ‚Äì Best Original Score for Evita (award shared with Tim Rice) *1983 ‚Äì Best Musical for Cats *1983 ‚Äì Best Original Score for Cats *1988 ‚Äì Best Musical for The Phantom of the Opera *1995 ‚Äì Best Musical for Sunset Boulevard *1995 ‚Äì Best Original Score for Sunset Boulevard  Olivier Awards  *1978 - Best Musical for Evita *1981 - Best Musical for Cats *1986 - Best Musical for The Phantom of the Opera *2008 - Society\\'s Special Award *Three other Production Awards  Other Awards  * 1988 - Drama Desk Award for Outstanding Orchestrations for The Phantom of the Opera * 1993 - Star on the Hollywood Walk of Fame for live theatre * 1995 - Praemium Imperiale * 1995 - Songwriter\\'s Hall of Fame * 2006 - Kennedy Center Honors * 2008 - Woodrow Wilson Award for Public Service  * 2009 - American Theatre Hall of Fame.  * 14 Ivor Novello Awards from the British Academy of Songwriters, Composers and Authors * 7 Laurence Olivier Awards (including Special Award presented for his 60th birthday in 2008)  Shows  Note: Music composed by Andrew Lloyd Webber unless otherwise noted.  * The Likes of Us (1965) Lyrics by Tim Rice Not produced until 2005 * Joseph and the Amazing Technicolor Dreamcoat (1968) Lyrics by Tim Rice * Jesus Christ Superstar (1970) Lyrics by Tim Rice * Jeeves (1975) Lyrics by Alan Ayckbourn Revised in 1996 as By Jeeves * Evita (1976) Lyrics by Tim Rice * Tell Me on a Sunday (1979) Lyrics by Don Black * Cats (1981) Lyrics based on Old Possum\\'s Book of Practical Cats by T. S. Eliot  Additional lyrics after Eliot by Richard Stilgoe and Trevor Nunn * Song and Dance (1982) Lyrics by Don Black (revised by Richard Maltby, Jr. for Broadway) Combination of Variations (1978) and Tell Me On A Sunday (1979) * Starlight Express (1984) Lyrics by Richard Stilgoe Later revisions by Don Black and David Yazbek Inspired by the Thomas the Tank Engine and Friends books by The Rev. W. Awdry. *Cricket (1986) Lyrics by Tim Rice First performed for Queen Elizabeth II\\'s 60th birthday * The Phantom of the Opera (1986) Lyrics by Charles Hart Additional Lyrics by Richard Stilgoe Based on the Gaston Leroux novel  * Aspects of Love (1989) Lyrics by Don Black and Charles Hart Based on the David Garnett novel * Sunset Boulevard (1993) Book and lyrics by Christopher Hampton and Don Black Based on the Billy Wilder film (1950) * Whistle Down the Wind (1996) Lyrics by Jim Steinman * The Beautiful Game (2000) Lyrics by Ben Elton Updated as The Boys in the Photograph (2009) * The Woman in White (2004) Lyrics by David Zippel Based on the Wilkie Collins novel * Love Never Dies (2010) Book & Lyrics by Glenn Slater Book by Ben Elton & Frederick Forsyth Additional lyrics by Charles Hart * The Wizard of Oz (2011) Adapted from the 1939 Motion Picture The Wizard of Oz Music by Harold Arlen Lyrics by E.Y. Harburg Additional music by Andrew Lloyd Webber Additional lyrics by Tim Rice * Stephen Ward the Musical (2013) Book and lyrics by Christopher Hampton and Don Black * School of Rock (2015) Lyrics by Glenn Slater Book by Julian Fellowes Based on the 2003 film  Film adaptations  There have been a number of film adaptations of the Lloyd Webber musicals. Jesus Christ Superstar (1973) was directed by Norman Jewison; Evita (1996) was directed by Alan Parker; and The Phantom of the Opera (2004) was directed by Joel Schumacher and co-produced by Lloyd Webber. Cats, Joseph and the Amazing Technicolor Dreamcoat, Jesus Christ Superstar and By Jeeves have been adapted into made for television films that have been released on DVD and VHS and often air on BBC.  A special performance of The Phantom of the Opera at the Royal Albert Hall for the 25th anniversary was broadcast live to cinemas in early October 2011 and later released on DVD and Blu-ray in February 2012. The same was also done with a reworked version of Love Never Dies. Filmed in Melbourne, Australia, it received a limited cinema release in the US and Canada in 2012, to see if it would be viable to bring the show to Broadway. It received positive reviews and was No.1 on DVD charts in the UK and Ireland, and did well in America.  In February 2014, it was announced that Elton John\\'s production company had acquired the rights to Joseph and the Amazing Technicolor Dreamcoat, and is planning to adapt it as a new theatrical animated musical film.   Other works  *Variations (1978) ‚Äì A set of musical variations on Niccol√≤ Paganini\\'s Caprice in A\\xa0minor that Lloyd Webber composed for his brother, cellist Julian. This album featured fifteen rock musicians including guitarist Gary Moore and pianist Rod Argent and reached number 2 in the UK album chart upon its release. It was later combined with Tell Me on a Sunday to form one show, Song and Dance. Lloyd Webber also used variation five as the basis for Unexpected Song in Song and Dance. The main theme is used as the theme music to The South Bank Show. *Requiem (1985) ‚Äì A classical choral work composed in honour of his father, William. *Watership Down (1999) - Lloyd Webber and Mike Batt, main soundtrack composer of the animated series adaptation of Richard Adams\\' novel of the same name, composed the song \"Fields of Sun\". The actual song was never used on the show, nor was it available on the CD soundtrack that was released at the time. He was however still credited for the unused song in the show\\'s opening titles.',\n",
              " 'norm_target': 'sunset boulevard',\n",
              " 'question': 'Which Lloyd Webber musical premiered in the US on 10th December 1993?',\n",
              " 'targets': ['Sunset Blvd',\n",
              "  'West Sunset Boulevard',\n",
              "  'Sunset Boulevard',\n",
              "  'Sunset Bulevard',\n",
              "  'Sunset Blvd.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqcviRfJw-3M",
        "colab_type": "text"
      },
      "source": [
        "Great, that is the structure we wanted! Some examples might have an empty evidence so we will filter those examples out.\n",
        "For this we can use the convenient `.filter()` function of `nlp`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0vSycFKxc0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec7b5428-676e-4e13-e40f-7a7a8e134a02"
      },
      "source": [
        "validation_dataset = validation_dataset.filter(lambda x: len(x[\"evidence\"]) > 0)\n",
        "# check out how many samples are left\n",
        "validation_dataset"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 20.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(schema: {'question': 'string', 'evidence': 'string', 'targets': 'list<item: string>', 'norm_target': 'string'}, num_rows: 187)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMh8EvQoxsrF",
        "colab_type": "text"
      },
      "source": [
        "Looks like all examples have an evidence in our case. Ok, let's think about the evaluation on **Longformer** now. \n",
        "\n",
        "On a usual 24GB GPU **Longformer** is able to process inputs of up to a length of **4096** tokens. As a rule of thumb, one token corresponds to more or less 4 characters for Longformer's word embeddings.\n",
        "Let's check how long *question* + *evidence* is in terms of characters for our examples and index each example with it's length. Again we can apply the convenient `.map()` function.\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: *Google Colab only offers GPUs with ~12 GB of RAM, so that we will set the max length to only 1024, which will obviously decrease performance of Longformer. A conventional 24 GB GPU (TITAN RTX) can fit up to a sequence length of 4096. So here we will also check how many examples have 4 * 1024 tokens or less* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ebRyrOmy4br",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6bcd7f31-035b-4eec-f555-b9eca7c4ec03"
      },
      "source": [
        "\n",
        "print(\"\\n\\nLength for each example\")\n",
        "print(30 * \"=\")\n",
        "\n",
        "# length for each example\n",
        "validation_dataset.map(lambda x, i: print(f\"Id: {i} - Question Length: {len(x['question'])} - Evidence Length: {len(x['evidence'])}\"), with_indices=True)\n",
        "print(30 * \"=\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Num examples larger than 4096 characters: \")\n",
        "# filter out examples smaller than 4 * 1024\n",
        "short_validation_examples = validation_dataset.filter(lambda x: (len(x['question']) + len(x['evidence'])) < 4 * 1024)\n",
        "short_validation_examples\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "187it [00:00, 4274.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Length for each example\n",
            "==============================\n",
            "Id: 0 - Question Length: 69 - Evidence Length: 31886\n",
            "Id: 0 - Question Length: 69 - Evidence Length: 31886\n",
            "Id: 0 - Question Length: 69 - Evidence Length: 31886\n",
            "Id: 1 - Question Length: 61 - Evidence Length: 92734\n",
            "Id: 2 - Question Length: 46 - Evidence Length: 2458\n",
            "Id: 3 - Question Length: 49 - Evidence Length: 33175\n",
            "Id: 4 - Question Length: 55 - Evidence Length: 27460\n",
            "Id: 5 - Question Length: 69 - Evidence Length: 95689\n",
            "Id: 6 - Question Length: 60 - Evidence Length: 80213\n",
            "Id: 7 - Question Length: 54 - Evidence Length: 40965\n",
            "Id: 8 - Question Length: 47 - Evidence Length: 8346\n",
            "Id: 9 - Question Length: 58 - Evidence Length: 66514\n",
            "Id: 10 - Question Length: 57 - Evidence Length: 43083\n",
            "Id: 11 - Question Length: 51 - Evidence Length: 823\n",
            "Id: 12 - Question Length: 48 - Evidence Length: 14555\n",
            "Id: 13 - Question Length: 55 - Evidence Length: 137066\n",
            "Id: 14 - Question Length: 64 - Evidence Length: 91380\n",
            "Id: 15 - Question Length: 79 - Evidence Length: 115129\n",
            "Id: 16 - Question Length: 100 - Evidence Length: 34736\n",
            "Id: 17 - Question Length: 57 - Evidence Length: 11291\n",
            "Id: 18 - Question Length: 80 - Evidence Length: 2339\n",
            "Id: 19 - Question Length: 75 - Evidence Length: 49982\n",
            "Id: 20 - Question Length: 116 - Evidence Length: 16115\n",
            "Id: 21 - Question Length: 81 - Evidence Length: 62202\n",
            "Id: 22 - Question Length: 45 - Evidence Length: 71824\n",
            "Id: 23 - Question Length: 70 - Evidence Length: 23077\n",
            "Id: 24 - Question Length: 50 - Evidence Length: 19289\n",
            "Id: 25 - Question Length: 49 - Evidence Length: 15348\n",
            "Id: 26 - Question Length: 52 - Evidence Length: 40966\n",
            "Id: 27 - Question Length: 52 - Evidence Length: 92451\n",
            "Id: 28 - Question Length: 96 - Evidence Length: 5310\n",
            "Id: 29 - Question Length: 42 - Evidence Length: 66143\n",
            "Id: 30 - Question Length: 38 - Evidence Length: 9151\n",
            "Id: 31 - Question Length: 48 - Evidence Length: 95128\n",
            "Id: 32 - Question Length: 57 - Evidence Length: 67669\n",
            "Id: 33 - Question Length: 43 - Evidence Length: 3521\n",
            "Id: 34 - Question Length: 47 - Evidence Length: 27681\n",
            "Id: 35 - Question Length: 38 - Evidence Length: 72219\n",
            "Id: 36 - Question Length: 59 - Evidence Length: 5469\n",
            "Id: 37 - Question Length: 37 - Evidence Length: 18247\n",
            "Id: 38 - Question Length: 71 - Evidence Length: 57522\n",
            "Id: 39 - Question Length: 50 - Evidence Length: 49267\n",
            "Id: 40 - Question Length: 46 - Evidence Length: 1017\n",
            "Id: 41 - Question Length: 33 - Evidence Length: 8851\n",
            "Id: 42 - Question Length: 46 - Evidence Length: 18591\n",
            "Id: 43 - Question Length: 65 - Evidence Length: 42637\n",
            "Id: 44 - Question Length: 52 - Evidence Length: 32864\n",
            "Id: 45 - Question Length: 43 - Evidence Length: 23118\n",
            "Id: 46 - Question Length: 51 - Evidence Length: 42435\n",
            "Id: 47 - Question Length: 55 - Evidence Length: 38324\n",
            "Id: 48 - Question Length: 50 - Evidence Length: 4771\n",
            "Id: 49 - Question Length: 84 - Evidence Length: 45955\n",
            "Id: 50 - Question Length: 56 - Evidence Length: 30291\n",
            "Id: 51 - Question Length: 54 - Evidence Length: 45300\n",
            "Id: 52 - Question Length: 70 - Evidence Length: 12214\n",
            "Id: 53 - Question Length: 64 - Evidence Length: 55067\n",
            "Id: 54 - Question Length: 68 - Evidence Length: 1849\n",
            "Id: 55 - Question Length: 40 - Evidence Length: 51632\n",
            "Id: 56 - Question Length: 47 - Evidence Length: 87972\n",
            "Id: 57 - Question Length: 29 - Evidence Length: 22272\n",
            "Id: 58 - Question Length: 48 - Evidence Length: 23099\n",
            "Id: 59 - Question Length: 67 - Evidence Length: 15243\n",
            "Id: 60 - Question Length: 67 - Evidence Length: 54203\n",
            "Id: 61 - Question Length: 62 - Evidence Length: 5865\n",
            "Id: 62 - Question Length: 53 - Evidence Length: 4787\n",
            "Id: 63 - Question Length: 58 - Evidence Length: 25293\n",
            "Id: 64 - Question Length: 76 - Evidence Length: 112048\n",
            "Id: 65 - Question Length: 26 - Evidence Length: 99217\n",
            "Id: 66 - Question Length: 65 - Evidence Length: 73779\n",
            "Id: 67 - Question Length: 33 - Evidence Length: 3724\n",
            "Id: 68 - Question Length: 51 - Evidence Length: 414\n",
            "Id: 69 - Question Length: 51 - Evidence Length: 22596\n",
            "Id: 70 - Question Length: 69 - Evidence Length: 28220\n",
            "Id: 71 - Question Length: 63 - Evidence Length: 12467\n",
            "Id: 72 - Question Length: 49 - Evidence Length: 3008\n",
            "Id: 73 - Question Length: 69 - Evidence Length: 22600\n",
            "Id: 74 - Question Length: 51 - Evidence Length: 50618\n",
            "Id: 75 - Question Length: 75 - Evidence Length: 15492\n",
            "Id: 76 - Question Length: 70 - Evidence Length: 36803\n",
            "Id: 77 - Question Length: 86 - Evidence Length: 183133\n",
            "Id: 78 - Question Length: 58 - Evidence Length: 6897\n",
            "Id: 79 - Question Length: 76 - Evidence Length: 43265\n",
            "Id: 80 - Question Length: 65 - Evidence Length: 137325\n",
            "Id: 81 - Question Length: 66 - Evidence Length: 7255\n",
            "Id: 82 - Question Length: 93 - Evidence Length: 124858\n",
            "Id: 83 - Question Length: 64 - Evidence Length: 4308\n",
            "Id: 84 - Question Length: 62 - Evidence Length: 11893\n",
            "Id: 85 - Question Length: 75 - Evidence Length: 12055\n",
            "Id: 86 - Question Length: 68 - Evidence Length: 13871\n",
            "Id: 87 - Question Length: 72 - Evidence Length: 4270\n",
            "Id: 88 - Question Length: 56 - Evidence Length: 40063\n",
            "Id: 89 - Question Length: 53 - Evidence Length: 24092\n",
            "Id: 90 - Question Length: 54 - Evidence Length: 56746\n",
            "Id: 91 - Question Length: 74 - Evidence Length: 15914\n",
            "Id: 92 - Question Length: 34 - Evidence Length: 43813\n",
            "Id: 93 - Question Length: 70 - Evidence Length: 49361\n",
            "Id: 94 - Question Length: 59 - Evidence Length: 15933\n",
            "Id: 95 - Question Length: 59 - Evidence Length: 64074\n",
            "Id: 96 - Question Length: 46 - Evidence Length: 2510\n",
            "Id: 97 - Question Length: 119 - Evidence Length: 105341\n",
            "Id: 98 - Question Length: 70 - Evidence Length: 12929\n",
            "Id: 99 - Question Length: 41 - Evidence Length: 126298\n",
            "Id: 100 - Question Length: 93 - Evidence Length: 2280\n",
            "Id: 101 - Question Length: 34 - Evidence Length: 25084\n",
            "Id: 102 - Question Length: 65 - Evidence Length: 85861\n",
            "Id: 103 - Question Length: 49 - Evidence Length: 70974\n",
            "Id: 104 - Question Length: 59 - Evidence Length: 48810\n",
            "Id: 105 - Question Length: 47 - Evidence Length: 3175\n",
            "Id: 106 - Question Length: 66 - Evidence Length: 84436\n",
            "Id: 107 - Question Length: 37 - Evidence Length: 16375\n",
            "Id: 108 - Question Length: 89 - Evidence Length: 5393\n",
            "Id: 109 - Question Length: 49 - Evidence Length: 17210\n",
            "Id: 110 - Question Length: 88 - Evidence Length: 20128\n",
            "Id: 111 - Question Length: 95 - Evidence Length: 11473\n",
            "Id: 112 - Question Length: 61 - Evidence Length: 77619\n",
            "Id: 113 - Question Length: 59 - Evidence Length: 79722\n",
            "Id: 114 - Question Length: 62 - Evidence Length: 64718\n",
            "Id: 115 - Question Length: 98 - Evidence Length: 93640\n",
            "Id: 116 - Question Length: 29 - Evidence Length: 40250\n",
            "Id: 117 - Question Length: 40 - Evidence Length: 148208\n",
            "Id: 118 - Question Length: 59 - Evidence Length: 13763\n",
            "Id: 119 - Question Length: 65 - Evidence Length: 3338\n",
            "Id: 120 - Question Length: 43 - Evidence Length: 27856\n",
            "Id: 121 - Question Length: 65 - Evidence Length: 54483\n",
            "Id: 122 - Question Length: 60 - Evidence Length: 11299\n",
            "Id: 123 - Question Length: 109 - Evidence Length: 249862\n",
            "Id: 124 - Question Length: 46 - Evidence Length: 114523\n",
            "Id: 125 - Question Length: 104 - Evidence Length: 33933\n",
            "Id: 126 - Question Length: 60 - Evidence Length: 48869\n",
            "Id: 127 - Question Length: 96 - Evidence Length: 32569\n",
            "Id: 128 - Question Length: 51 - Evidence Length: 29176\n",
            "Id: 129 - Question Length: 39 - Evidence Length: 4004\n",
            "Id: 130 - Question Length: 60 - Evidence Length: 114038\n",
            "Id: 131 - Question Length: 93 - Evidence Length: 127480\n",
            "Id: 132 - Question Length: 66 - Evidence Length: 91607\n",
            "Id: 133 - Question Length: 72 - Evidence Length: 2196\n",
            "Id: 134 - Question Length: 86 - Evidence Length: 14021\n",
            "Id: 135 - Question Length: 71 - Evidence Length: 17171\n",
            "Id: 136 - Question Length: 61 - Evidence Length: 51173\n",
            "Id: 137 - Question Length: 51 - Evidence Length: 95689\n",
            "Id: 138 - Question Length: 55 - Evidence Length: 63561\n",
            "Id: 139 - Question Length: 51 - Evidence Length: 1837\n",
            "Id: 140 - Question Length: 74 - Evidence Length: 80462\n",
            "Id: 141 - Question Length: 53 - Evidence Length: 46131\n",
            "Id: 142 - Question Length: 63 - Evidence Length: 79823\n",
            "Id: 143 - Question Length: 55 - Evidence Length: 43998\n",
            "Id: 144 - Question Length: 68 - Evidence Length: 66836\n",
            "Id: 145 - Question Length: 69 - Evidence Length: 27240\n",
            "Id: 146 - Question Length: 55 - Evidence Length: 7452\n",
            "Id: 147 - Question Length: 45 - Evidence Length: 52445\n",
            "Id: 148 - Question Length: 27 - Evidence Length: 13281\n",
            "Id: 149 - Question Length: 86 - Evidence Length: 144346\n",
            "Id: 150 - Question Length: 39 - Evidence Length: 64464\n",
            "Id: 151 - Question Length: 39 - Evidence Length: 150230\n",
            "Id: 152 - Question Length: 45 - Evidence Length: 126888\n",
            "Id: 153 - Question Length: 41 - Evidence Length: 28775\n",
            "Id: 154 - Question Length: 59 - Evidence Length: 39631\n",
            "Id: 155 - Question Length: 93 - Evidence Length: 8391\n",
            "Id: 156 - Question Length: 51 - Evidence Length: 22489\n",
            "Id: 157 - Question Length: 50 - Evidence Length: 78665\n",
            "Id: 158 - Question Length: 59 - Evidence Length: 25629\n",
            "Id: 159 - Question Length: 82 - Evidence Length: 174487\n",
            "Id: 160 - Question Length: 109 - Evidence Length: 16814\n",
            "Id: 161 - Question Length: 60 - Evidence Length: 237243\n",
            "Id: 162 - Question Length: 43 - Evidence Length: 166433\n",
            "Id: 163 - Question Length: 56 - Evidence Length: 105678\n",
            "Id: 164 - Question Length: 57 - Evidence Length: 86628\n",
            "Id: 165 - Question Length: 79 - Evidence Length: 115092\n",
            "Id: 166 - Question Length: 85 - Evidence Length: 226600\n",
            "Id: 167 - Question Length: 66 - Evidence Length: 123074\n",
            "Id: 168 - Question Length: 54 - Evidence Length: 144413\n",
            "Id: 169 - Question Length: 126 - Evidence Length: 83264\n",
            "Id: 170 - Question Length: 72 - Evidence Length: 79295\n",
            "Id: 171 - Question Length: 52 - Evidence Length: 12264\n",
            "Id: 172 - Question Length: 76 - Evidence Length: 37495\n",
            "Id: 173 - Question Length: 91 - Evidence Length: 4532\n",
            "Id: 174 - Question Length: 84 - Evidence Length: 122248\n",
            "Id: 175 - Question Length: 54 - Evidence Length: 24062\n",
            "Id: 176 - Question Length: 43 - Evidence Length: 57187\n",
            "Id: 177 - Question Length: 38 - Evidence Length: 42894\n",
            "Id: 178 - Question Length: 66 - Evidence Length: 30129\n",
            "Id: 179 - Question Length: 61 - Evidence Length: 18605\n",
            "Id: 180 - Question Length: 109 - Evidence Length: 126892\n",
            "Id: 181 - Question Length: 85 - Evidence Length: 59900\n",
            "Id: 182 - Question Length: 98 - Evidence Length: 48489\n",
            "Id: 183 - Question Length: 87 - Evidence Length: 38068\n",
            "Id: 184 - Question Length: 59 - Evidence Length: 6076\n",
            "Id: 185 - Question Length: 104 - Evidence Length: 78919\n",
            "Id: 186 - Question Length: 104 - Evidence Length: 43858\n",
            "==============================\n",
            "\n",
            "\n",
            "Num examples larger than 4096 characters: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(schema: {'question': 'string', 'evidence': 'string', 'targets': 'list<item: string>', 'norm_target': 'string'}, num_rows: 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5MyFEZqzwZ-",
        "colab_type": "text"
      },
      "source": [
        "Interesting! We can see that only 16 examples have less than 4096 characters...\n",
        "\n",
        "Most examples seem to have a very long evidence which will have to be cut to to Longformer's maximum length.\n",
        "\n",
        "Let's write our evaluation function and import a pretrained `LongformerForQuestionAnswering`. For more details on the `LongformerForQuestionAnswering` model, see [here](https://huggingface.co/transformers/model_doc/longformer.html?highlight=longformerforquestionanswering#transformers.LongformerForQuestionAnswering}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgcdBt41gby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
        "\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n",
        "\n",
        "# download the 1.7 GB pretrained model. It might take ~1min\n",
        "model = LongformerForQuestionAnswering.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "def evaluate(example):\n",
        "    def get_answer(question, evidence):\n",
        "        # encode question and evidence so that they are seperated by a tokenizer.sep_token and cut at max_length\n",
        "        encoding = tokenizer.encode_plus(question, evidence, return_tensors=\"pt\", max_length=1024)\n",
        "        input_ids = encoding[\"input_ids\"].to(\"cuda\")\n",
        "        attention_mask = encoding[\"attention_mask\"].to(\"cuda\")\n",
        "\n",
        "        # the forward method will automatically set global attention on question tokens\n",
        "        # The scores for the possible start token and end token of the answer are retrived\n",
        "        start_scores, end_scores = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Let's take the most likely token using `argmax` and retrieve the answer\n",
        "        all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
        "        answer_tokens = all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1]\n",
        "        answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))[1:].replace('\"', '')  # remove space prepending space token and remove unnecessary '\"'\n",
        "        \n",
        "        return answer\n",
        "\n",
        "    # save the model's output here\n",
        "    example[\"output\"] = get_answer(example[\"question\"], example[\"evidence\"])\n",
        "\n",
        "    # save if it's a match or not\n",
        "    example[\"match\"] = (example[\"output\"] in example[\"targets\"])\n",
        "\n",
        "    return example\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15ia0-LVRKt",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate two datasets:\n",
        "- One that includes only examples with less then 4096 characters and \n",
        "- Another one that includes all examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2REN9qH4HR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f50be6e1-b10b-4b5d-fbd8-e13f8e784721"
      },
      "source": [
        "import torch\n",
        "results_short = short_validation_examples.map(evaluate)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16it [00:03,  4.12it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULyIPLZIbtIP",
        "colab_type": "text"
      },
      "source": [
        "Now let's check for how many questions we were correct!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kavPCIWKWzua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "475c6cc0-170b-41e0-a391-703321f2fb40"
      },
      "source": [
        "print(f\"\\nNum Correct examples: {sum(results_short['match'])}/{len(results_short)}\")\n",
        "wrong_results = results_short.filter(lambda x: x['match'] is False)\n",
        "print(f\"\\nWrong examples: \")\n",
        "wrong_results.map(lambda x, i: print(f\"{i} - Output: {x['output']} - Target: {x['norm_target']}\"), with_indices=True)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5it [00:00, 3460.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Num Correct examples: 11/16\n",
            "\n",
            "Wrong examples: \n",
            "0 - Output:  - Target: mutiny on bounty\n",
            "0 - Output:  - Target: mutiny on bounty\n",
            "0 - Output:  - Target: mutiny on bounty\n",
            "1 - Output: Roy Orbison - Target: donny osmond\n",
            "2 - Output: Gary Lewis - Target: gary lewis and playboys\n",
            "3 - Output: Collapsible baby buggy - Target: baby buggy\n",
            "4 - Output:  - Target: basket ball\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(schema: {'question': 'string', 'evidence': 'string', 'targets': 'list<item: string>', 'norm_target': 'string', 'output': 'string', 'match': 'bool'}, num_rows: 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViCCSxzPc2Gk",
        "colab_type": "text"
      },
      "source": [
        "11/16 is not bad! Also we can see that two of the wrong outputs are very close to the correct solution (Number 2 and 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5CtQILEVMay",
        "colab_type": "text"
      },
      "source": [
        "Second, we evaluate `LongformerForQuestionAnswering` on the short examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWNtljj3Wr9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a11db441-31ac-4680-fba2-9cd7339f6ede"
      },
      "source": [
        "results = validation_dataset.map(evaluate)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "187it [01:54,  1.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyDYG4YDXFV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "675e54a0-e88f-4633-d5d2-6f4cd4ed93dc"
      },
      "source": [
        "print(f\"Correct examples: {sum(results['match'])}/{len(results)}\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct examples: 75/187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80MqFyP4JIj",
        "colab_type": "text"
      },
      "source": [
        "Here, we now see a clear degradation. Only less than half the samples are correct. Still 75 out of 187 is a good score, also given that we can only use 1024 tokens in this notebook."
      ]
    }
  ]
}